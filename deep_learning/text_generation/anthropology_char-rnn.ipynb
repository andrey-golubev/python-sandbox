{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char RNN producing anthropological statements\n",
    "\n",
    "Global target:\n",
    "\n",
    "* Learn model on [anthropology book](https://www.gutenberg.org/ebooks/17280)\n",
    "* Create an alternate knowledge about humans, their behavior, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"Read file\"\"\"\n",
    "    f = unidecode.unidecode(open(filename).read())\n",
    "    return f, len(f)\n",
    "\n",
    "def char_tensor(string):\n",
    "    \"\"\"String to tensor\"\"\"\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor)\n",
    "\n",
    "def elapsed(start):\n",
    "    \"\"\"Get elapsed time\"\"\"\n",
    "    secs = time.time() - start\n",
    "    mins = math.floor(secs / 60)\n",
    "    secs -= mins * 60\n",
    "    return '{}m {}s'.format(mins, secs)\n",
    "\n",
    "def random_chunk(size):\n",
    "    start_index = random.randint(0, file_len - size)\n",
    "    end_index = start_index + size + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "def random_training_set(size=200, verbose=False):    \n",
    "    chunk = random_chunk(size)\n",
    "    if verbose:\n",
    "        print(chunk)\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.abspath('_data/anthropology.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 319704\n",
      "\"Bone of our bone, and flesh of our flesh, are these half-brutish\n",
      "prehistoric brothers. Girdled abou\n"
     ]
    }
   ],
   "source": [
    "file, file_len = read_file(data_path)\n",
    "print('file_len =', file_len)\n",
    "print(file[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e forms of social\\norganization prevailing amongst peoples of the lower culture. Our\\ninterest will be confined to the social morphology. In subsequent\\nchapters we shall go on to what might be called, by'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_chunk(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 24,  16,  34,  88,  71,  94,  69,  11,  14,  28,  29,  94,\n",
       "          22,  10,  23,  30,  10,  21,  70,  78,  94,  88,  51,  27,\n",
       "          18,  22,  18,  29,  18,  31,  14,  96,  38,  30,  21,  29,\n",
       "          30,  27,  14,  88,  71,  94,  69,  29,  17,  14,  94,  16,\n",
       "          27,  14]),\n",
       " tensor([ 16,  34,  88,  71,  94,  69,  11,  14,  28,  29,  94,  22,\n",
       "          10,  23,  30,  10,  21,  70,  78,  94,  88,  51,  27,  18,\n",
       "          22,  18,  29,  18,  31,  14,  96,  38,  30,  21,  29,  30,\n",
       "          27,  14,  88,  71,  94,  69,  29,  17,  14,  94,  16,  27,\n",
       "          14,  10]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_training_set(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(decoder, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(decoder, decoder_optimizer, inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output, target[c].unsqueeze(0))\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 56.484047651290894s (100 5%) 2.1665]\n",
      "Wher wore orighlogen a condilgult forn the scor wore beeed of ouldle conter or oral wither, with ext i \n",
      "\n",
      "[1m 52.279730558395386s (200 10%) 1.9199]\n",
      "Why we hough the the clauszethed out forms--the the they the at conots the worly and forms the routo m \n",
      "\n",
      "[2m 47.5268759727478s (300 15%) 2.0104]\n",
      "Whited. Andery suith of stect inciably, nitertai, Whight whoweth\n",
      "one\n",
      "formy howgly somelss feethowent,  \n",
      "\n",
      "[3m 42.09211826324463s (400 20%) 2.3259]\n",
      "When the and the have lart-cagerseas with there from the tige twhe regation elaled, and in the strese  \n",
      "\n",
      "[4m 37.21531057357788s (500 25%) 1.9142]\n",
      "Wher is that is seonioly\n",
      "the corking in agos. I day ore of of meastion cosastosinize deassy antralizal \n",
      "\n",
      "[5m 32.183722257614136s (600 30%) 1.7098]\n",
      "Wherian is religion in the offer aming and the relagions of\n",
      "denderia-cultions of the with enders overa \n",
      "\n",
      "[6m 26.69851279258728s (700 35%) 1.6921]\n",
      "Whip that of \"tain it is peter of somone amprosing of sepativer, it\n",
      "sidety is is we a simpace of eveve \n",
      "\n",
      "[7m 24.68099069595337s (800 40%) 1.7638]\n",
      "Whit intribly crective\n",
      "for shaffectlitive the make the can the cast vary, and what a he *        *     \n",
      "\n",
      "[8m 20.769920110702515s (900 45%) 1.6600]\n",
      "What of and will be lielogs the amin their one's with and a whend and the were werate is ininterersent \n",
      "\n",
      "[9m 17.694868564605713s (1000 50%) 1.9354]\n",
      "Whid so mustory, of abost the words that to the show to livet, I so the us not hish howevers all to li \n",
      "\n",
      "[10m 14.386009454727173s (1100 55%) 2.0369]\n",
      "Whic I pome hery are on good are onor the have vood, they compenter of the clives, ageds compler. They \n",
      "\n",
      "[11m 9.704562425613403s (1200 60%) 1.9639]\n",
      "Whas wowns of to are type, the when of tywould rear would, the wis deen puresed the with other of they \n",
      "\n",
      "[12m 6.007007122039795s (1300 65%) 1.7458]\n",
      "What as way at of at the coloust as wortilangs of as word and by exause of an is of most to and some t \n",
      "\n",
      "[13m 3.0741891860961914s (1400 70%) 2.0586]\n",
      "Whinded the perequeon my tome I to get how depnsts\n",
      "of the dature regions, to the allike work down the  \n",
      "\n",
      "[13m 58.1414909362793s (1500 75%) 1.8423]\n",
      "Whratinguent urpural ty it to be whild howeverly of purplitive and the forth seepply guan\n",
      "interle of t \n",
      "\n",
      "[14m 54.18439078330994s (1600 80%) 1.7423]\n",
      "Whise manstor a calials indivilly we cave apalized miver supprertwer the\n",
      "willge, alimate succums to or \n",
      "\n",
      "[15m 49.42296481132507s (1700 85%) 1.6410]\n",
      "Whild please forms of the speat, wore expence.\n",
      "\n",
      "I I apen inity the ince that way of serpession implali \n",
      "\n",
      "[16m 44.833335876464844s (1800 90%) 1.6287]\n",
      "Whinace in the compleasion, certionmion of their plaining comming Not\n",
      "beince to their buts releather i \n",
      "\n",
      "[17m 40.1481990814209s (1900 95%) 1.9386]\n",
      "Why the for hilly to may marn we fine of two the fine--great and we wes the\n",
      "mant the tiction a pare a  \n",
      "\n",
      "[18m 35.06161379814148s (2000 100%) 1.5358]\n",
      "Whinve as factive\n",
      "folle, but heremet there\n",
      "remelemaly know even well, when one whol,, we concral treed \n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"hidden_size\": 50,\n",
    "    \"n_layers\": 2,\n",
    "    \"lr\": 0.005,\n",
    "    \"n_epochs\": 2000,\n",
    "    \"print_every\": 100,\n",
    "    \"plot_every\": 10,\n",
    "    \"hidden_size\": 300,\n",
    "    \"chunk_len\": 200,\n",
    "}\n",
    "\n",
    "n_epochs = args[\"n_epochs\"]\n",
    "chunk_len = args[\"chunk_len\"]\n",
    "\n",
    "decoder = RNN(n_characters, args[\"hidden_size\"], n_characters, args[\"n_layers\"])\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=args[\"lr\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set(chunk_len))\n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % args[\"print_every\"] == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (elapsed(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "    if epoch % args[\"plot_every\"] == 0:\n",
    "        all_losses.append(loss_avg / args[\"plot_every\"])\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_by_params(args, initial='A', predict=100, decoder=None):\n",
    "    n_epochs = args[\"n_epochs\"]\n",
    "    chunk_len = args[\"chunk_len\"]\n",
    "\n",
    "    if not decoder:\n",
    "        decoder = RNN(n_characters, args[\"hidden_size\"], n_characters, args[\"n_layers\"])\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=args[\"lr\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start = time.time()\n",
    "    all_losses = []\n",
    "    loss_avg = 0\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train(decoder, decoder_optimizer, *random_training_set(chunk_len))\n",
    "        loss_avg += loss\n",
    "\n",
    "        if epoch % args[\"print_every\"] == 0:\n",
    "            print('[%s (%d %d%%) %.4f]' % (elapsed(start), epoch, epoch / n_epochs * 100, loss))\n",
    "            print(evaluate(decoder, initial, predict), '\\n')\n",
    "\n",
    "        if epoch % args[\"plot_every\"] == 0:\n",
    "            all_losses.append(loss_avg / args[\"plot_every\"])\n",
    "            loss_avg = 0\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11m 21.715171098709106s (1000 20%) 2.1840]\n",
      "Whp.,\n",
      "\n",
      "Whark, be of racial cust be ulhalf ound reling in hard to presels the\n",
      "manimil\n",
      "orgh in naste mut \n",
      "\n",
      "[23m 23.652442455291748s (2000 40%) 1.8533]\n",
      "Whensegstionts, then, an mankone monesain--. Atcontrancainout, The call noth mecharly of can any can-q \n",
      "\n",
      "[35m 21.067936658859253s (3000 60%) 1.8423]\n",
      "Wh at that the that somong I out of the for the rost\n",
      "of thetough it came it a conly\n",
      "can see ture as a  \n",
      "\n",
      "[47m 11.544966220855713s (4000 80%) 1.8795]\n",
      "Whion which a so most of do do or not a make a                              the of ame how for the mon \n",
      "\n",
      "[59m 18.745714902877808s (5000 100%) 2.1755]\n",
      "Whe bother holding mus bod of whoughoung all to as to own and out on llow docte wot and wo way other o \n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"n_layers\": 5,\n",
    "    \"lr\": 0.01,\n",
    "    \"n_epochs\": 5000,\n",
    "    \"print_every\": 1000,\n",
    "    \"plot_every\": 100,\n",
    "    \"hidden_size\": 500,\n",
    "    \"chunk_len\": 1000,\n",
    "}\n",
    "\n",
    "decoder1 = train_by_params(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12m 2.6792047023773193s (1000 50%) 2.0174]\n",
      "Whem carmice-he fid cond in carting the conciencall conning isistich is of when custh the eclick incly \n",
      "\n",
      "[23m 58.59296250343323s (2000 100%) 1.8807]\n",
      "Wh past man from Mleences that genec to the geent Mongonirution digrals boff M geance deeren of the sa \n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"n_layers\": 5,\n",
    "    \"lr\": 0.01,\n",
    "    \"n_epochs\": 2000,\n",
    "    \"print_every\": 1000,\n",
    "    \"plot_every\": 100,\n",
    "    \"hidden_size\": 300,\n",
    "    \"chunk_len\": 500,\n",
    "}\n",
    "\n",
    "decoder2 = train_by_params(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 40.80359864234924s (100 10%) 2.2717]\n",
      "Language is coring ances fean in cioll-can and plaalt And in f \n",
      "\n",
      "[1m 17.814353227615356s (200 20%) 2.1049]\n",
      "Language is bint; a tomore of other? Is inded muthorateont of  \n",
      "\n",
      "[1m 53.368446350097656s (300 30%) 2.2245]\n",
      "Language is the riftion the ond ow as as socan, wo we sufing t \n",
      "\n",
      "[2m 28.799882173538208s (400 40%) 2.2195]\n",
      "Language is the perizing with into thell is pidem all, the\n",
      "pup \n",
      "\n",
      "[3m 4.968152046203613s (500 50%) 2.5396]\n",
      "Language is this hiple may with at of wompliget be a amiss to  \n",
      "\n",
      "[3m 40.475257396698s (600 60%) 2.0204]\n",
      "Language is of the typleinale of thesee of must theyle of a an \n",
      "\n",
      "[4m 18.894949674606323s (700 70%) 2.0521]\n",
      "Language is leuman 100. ETheand for say the jething a sastande \n",
      "\n",
      "[4m 55.964497327804565s (800 80%) 1.7335]\n",
      "Language is a succt-which lew les chesisaans yo the by out the \n",
      "\n",
      "[5m 31.76429510116577s (900 90%) 2.1737]\n",
      "Language is ceternedion tilf fhouthord, hared not chaien are t \n",
      "\n",
      "[6m 9.840324640274048s (1000 100%) 2.3158]\n",
      "Language is a not it actiolh at ther all to at ermplalfaring a \n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"hidden_size\": 50,\n",
    "    \"n_layers\": 2,\n",
    "    \"lr\": 0.005,\n",
    "    \"n_epochs\": 1000,\n",
    "    \"print_every\": 100,\n",
    "    \"plot_every\": 10,\n",
    "    \"hidden_size\": 300,\n",
    "    \"chunk_len\": 100,\n",
    "}\n",
    "\n",
    "n_epochs = args[\"n_epochs\"]\n",
    "chunk_len = args[\"chunk_len\"]\n",
    "\n",
    "decoder = RNN(n_characters, args[\"hidden_size\"], n_characters, args[\"n_layers\"])\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=args[\"lr\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set(chunk_len))\n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % args[\"print_every\"] == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (elapsed(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(evaluate('Language is ', 50), '\\n')\n",
    "\n",
    "    if epoch % args[\"plot_every\"] == 0:\n",
    "        all_losses.append(loss_avg / args[\"plot_every\"])\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 32.05202889442444s (100 5%) 2.3469]\n",
      "Language is is ory tis, id. anom the ther, Giens in in the so \n",
      "\n",
      "[1m 3.129647970199585s (200 10%) 2.1517]\n",
      "Language is whethere\n",
      "wilade, Weise theefentaaser inerin,\n",
      "he\n",
      "w \n",
      "\n",
      "[1m 34.14811038970947s (300 15%) 2.1172]\n",
      "Language istele wiltitheding thee wenot hit wilust ath-reing  \n",
      "\n",
      "[2m 5.658927917480469s (400 20%) 2.1291]\n",
      "Language is tow the pares, as an any suves. Ip patian the pru \n",
      "\n",
      "[2m 36.50493144989014s (500 25%) 2.0236]\n",
      "Language is the ofnet ancience, of lantsinens apppentines.\n",
      "Nh \n",
      "\n",
      "[3m 8.349589109420776s (600 30%) 2.0715]\n",
      "Language is nother, gount\n",
      "thather, Geny\n",
      "showe a suros a ben i \n",
      "\n",
      "[3m 39.20461654663086s (700 35%) 2.0619]\n",
      "Language is wow be to coled theremontwer to dee. It nay backs \n",
      "\n",
      "[4m 9.97642207145691s (800 40%) 2.0593]\n",
      "Language istrem in and the orge and in\n",
      "the drely ban\n",
      "an nown  \n",
      "\n",
      "[4m 40.84849548339844s (900 45%) 2.0913]\n",
      "Language is meacae sack of cominenrence or sourd prome funim, \n",
      "\n",
      "[5m 11.769697904586792s (1000 50%) 1.9120]\n",
      "Language is in in by into cour the whe so of cous, in bory on \n",
      "\n",
      "[5m 42.71797299385071s (1100 55%) 1.9967]\n",
      "Language is Powent, phesel feesceances hiser, of ano his bene \n",
      "\n",
      "[6m 13.718388080596924s (1200 60%) 2.0521]\n",
      "Language is pripposing is condes in  skiligion, to segh escer \n",
      "\n",
      "[6m 45.094799518585205s (1300 65%) 1.8592]\n",
      "Language is anastorse\" hics a hasinie in kusound the self the \n",
      "\n",
      "[7m 17.638035774230957s (1400 70%) 1.9828]\n",
      "Language is the seemes curdiffening of phile\n",
      "usters if\n",
      "of the \n",
      "\n",
      "[7m 48.73754072189331s (1500 75%) 1.8055]\n",
      "Language is we some this sturits of mas aner teresions the th \n",
      "\n",
      "[8m 19.23878502845764s (1600 80%) 1.9621]\n",
      "Language is mut sout any inse use inster Rother releing the r \n",
      "\n",
      "[8m 49.82416319847107s (1700 85%) 2.0935]\n",
      "Language is of those ant\n",
      "thoweventoly moran to ofmorstoloped  \n",
      "\n",
      "[9m 20.521771907806396s (1800 90%) 2.0831]\n",
      "Language is mater there in of roth of longethroling the in th \n",
      "\n",
      "[9m 51.5328369140625s (1900 95%) 1.9609]\n",
      "Language is pacintises\n",
      "pists in in in this with is\n",
      "sealtised  \n",
      "\n",
      "[10m 22.14916968345642s (2000 100%) 2.1992]\n",
      "Language is lown, to\n",
      "allimsem maure aration mae a dure' is ma \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (encoder): Embedding(100, 300)\n",
       "  (gru): GRU(300, 300, num_layers=2)\n",
       "  (decoder): Linear(in_features=300, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {\n",
    "    \"hidden_size\": 50,\n",
    "    \"n_layers\": 2,\n",
    "    \"lr\": 0.005,\n",
    "    \"n_epochs\": 2000,\n",
    "    \"print_every\": 100,\n",
    "    \"plot_every\": 10,\n",
    "    \"hidden_size\": 300,\n",
    "    \"chunk_len\": 200,\n",
    "}\n",
    "train_by_params(args, 'Language is', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1m 5.41919469833374s (200 20%) 2.2230]\n",
      "Language is. Lan sowet as and minog to wan way the we stoweve \n",
      "\n",
      "[2m 9.47817611694336s (400 40%) 2.1557]\n",
      "Language is the\n",
      "marisse whatiblount thin\n",
      "that is chome is the \n",
      "\n",
      "[3m 11.14742112159729s (600 60%) 1.7708]\n",
      "Language is as caing, iningresicines to as the\n",
      "cover\n",
      "ones pop \n",
      "\n",
      "[4m 14.249257802963257s (800 80%) 2.2481]\n",
      "Language is be leanduns, of dienellky bentoing and me of vill \n",
      "\n",
      "[5m 17.714370489120483s (1000 100%) 2.0607]\n",
      "Language is famith of the pothes, of to of the\n",
      "An or, manis a \n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"hidden_size\": 50,\n",
    "    \"n_layers\": 2,\n",
    "    \"lr\": 0.005,\n",
    "    \"n_epochs\": 1000,\n",
    "    \"print_every\": 200,\n",
    "    \"plot_every\": 10,\n",
    "    \"hidden_size\": 300,\n",
    "    \"chunk_len\": 200,\n",
    "}\n",
    "d = train_by_params(args, 'Language is', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
