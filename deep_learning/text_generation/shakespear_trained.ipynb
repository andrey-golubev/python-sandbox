{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"Read file\"\"\"\n",
    "    f = unidecode.unidecode(open(filename).read())\n",
    "    return f, len(f)\n",
    "\n",
    "def char_tensor(string):\n",
    "    \"\"\"String to tensor\"\"\"\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor)\n",
    "\n",
    "def elapsed(start):\n",
    "    \"\"\"Get elapsed time\"\"\"\n",
    "    secs = time.time() - start\n",
    "    mins = math.floor(secs / 60)\n",
    "    secs -= mins * 60\n",
    "    return '{}m {}s'.format(mins, secs)\n",
    "\n",
    "def random_chunk(size):\n",
    "    start_index = random.randint(0, file_len - size)\n",
    "    end_index = start_index + size + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "def random_training_set(size=200, verbose=False):    \n",
    "    chunk = random_chunk(size)\n",
    "    if verbose:\n",
    "        print(chunk)\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.abspath('_data/tiny-shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "file, file_len = read_file(data_path)\n",
    "print('file_len =', file_len)\n",
    "print(file[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"d learn this lesson, draw thy sword in right.\\n\\nPRINCE:\\nMy gracious father, by your kingly leave,\\nI'll draw it as apparent to the crown,\\nAnd in that quarrel use it to the death.\\n\\nCLIFFORD:\\nWhy, that is \""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_chunk(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 18,  35,  14,  94,  29,  32,  24,  94,  22,  14,  10,  23,\n",
       "          18,  23,  16,  28,  94,  18,  23,  94,  24,  23,  14,  94,\n",
       "          32,  24,  27,  13,  75,  96,  96,  51,  53,  44,  49,  38,\n",
       "          40,  94,  40,  39,  58,  36,  53,  39,  77,  96,  55,  17,\n",
       "          10,  29]),\n",
       " tensor([ 35,  14,  94,  29,  32,  24,  94,  22,  14,  10,  23,  18,\n",
       "          23,  16,  28,  94,  18,  23,  94,  24,  23,  14,  94,  32,\n",
       "          24,  27,  13,  75,  96,  96,  51,  53,  44,  49,  38,  40,\n",
       "          94,  40,  39,  58,  36,  53,  39,  77,  96,  55,  17,  10,\n",
       "          29,  94]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_training_set(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "#         print(output, torch.from_numpy(np.array([target[c].data.numpy()])))\n",
    "        loss += criterion(output, torch.from_numpy(np.array([target[c].data.numpy()])))\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / chunk_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Professional\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 12.960622549057007s (100 5%) 2.2937]\n",
      "Wh, gin aut gerfert Lor,.\n",
      "\n",
      " lis ligh reto the soneen.\n",
      "\n",
      "IS\n",
      "DTESEELNENIESSE::\n",
      "Hort shhe enout to thard b \n",
      "\n",
      "[0m 26.145856618881226s (200 10%) 2.1413]\n",
      "Wh's and of hous in my krille ante for rone, the ofrave and and sidist nord shorlene; him cret me beng \n",
      "\n",
      "[0m 39.15329909324646s (300 15%) 2.2238]\n",
      "Wh nut gathy for a the as busirding and mis ar you sests sand!\n",
      "\n",
      "LULUTENTARD IINTIO Dento the an nony b \n",
      "\n",
      "[0m 52.09091377258301s (400 20%) 2.0840]\n",
      "Whous theere'd\n",
      "We thime the hear,\n",
      "Whor by his ther hares\n",
      "Wither thereing she fiir, im-Buth gitt\n",
      "Thaf m \n",
      "\n",
      "[1m 5.035867691040039s (500 25%) 1.8629]\n",
      "What the liettlerd,\n",
      "And thou shis caignomy Il beards! 'tet yous buders.\n",
      "\n",
      "MENCENTIO:\n",
      "And thy you loft o \n",
      "\n",
      "[1m 18.007661819458008s (600 30%) 2.0541]\n",
      "Wh encerd'd as the rujdor myre gruntle:\n",
      "Wherjod hin now Thy my spollal.\n",
      "\n",
      "KENANIUSS:\n",
      "Wourth faul god; h \n",
      "\n",
      "[1m 31.101768732070923s (700 35%) 1.9145]\n",
      "Where and mans;\n",
      "Brate thave in besif not lade the his nace\n",
      "Be for be erp is and fmury, so mord to word \n",
      "\n",
      "[1m 44.683717012405396s (800 40%) 2.0029]\n",
      "Whit servest, chems\n",
      "Whothen bored wam if heerire, alling prom her twale,\n",
      "in will is he musban, for wha \n",
      "\n",
      "[1m 58.552085399627686s (900 45%) 1.7579]\n",
      "Whell wiip rian peedant.\n",
      "That porteres that his to suke this pear.\n",
      "\n",
      "TRY VIN\n",
      "ENTIO:\n",
      "Vi to woold bow's f \n",
      "\n",
      "[2m 11.998332023620605s (1000 50%) 1.7913]\n",
      "Whellaino, theep you brotheries calls.\n",
      "\n",
      "SICINIUS:\n",
      "Ceak the vare, thou that in it meser of waded cimes. \n",
      "\n",
      "[2m 25.425026178359985s (1100 55%) 1.9169]\n",
      "Wher sill your yous. I hath conce baus's.\n",
      "\n",
      "ARD IV:\n",
      "We her Cleardestess befell then:\n",
      "And thou he homven \n",
      "\n",
      "[2m 38.84369921684265s (1200 60%) 2.0833]\n",
      "Where I near staties must,\n",
      "What know heaver she beet his I your ames trusine,\n",
      "And mysivip to then dear \n",
      "\n",
      "[2m 52.29947090148926s (1300 65%) 1.8054]\n",
      "Wh vostand though thy broth\n",
      "They love wut your the freated be'd give though down'd.\n",
      "\n",
      "BAMVOLANUS:\n",
      "What  \n",
      "\n",
      "[3m 5.726666688919067s (1400 70%) 2.0262]\n",
      "Wht how mast mempriet than this for godad my there\n",
      "The chavistion heil she fid for ervinused thee was  \n",
      "\n",
      "[3m 19.219537258148193s (1500 75%) 1.6334]\n",
      "Whering to rather with of the daralled.\n",
      "\n",
      "QUEEN ELIZESS VINCELDSIO:\n",
      "Well all'd anvy thee shem?\n",
      "\n",
      "KINCENT \n",
      "\n",
      "[3m 32.64472794532776s (1600 80%) 2.0318]\n",
      "Whus the soul deat that too warn is madesed of the lencounches?\n",
      "\n",
      "Sheven exerence cane the give is ccom \n",
      "\n",
      "[3m 46.087966203689575s (1700 85%) 1.8226]\n",
      "Whe forth with he feef alf, but this old grew;\n",
      "Wathe stand that chares that and stong to his have are  \n",
      "\n",
      "[3m 59.473050117492676s (1800 90%) 1.8358]\n",
      "Who eforte to and day\n",
      "Ones us I knight a infinot there,\n",
      "The son sthe is henent.\n",
      "\n",
      "PETRUCHIO:\n",
      "For the gi \n",
      "\n",
      "[4m 12.899744272232056s (1900 95%) 1.6739]\n",
      "Whinvath\n",
      "Look with thou perciout fute that a shoo my dono his loods's there!\n",
      "\n",
      "This were a with in that \n",
      "\n",
      "[4m 26.322432041168213s (2000 100%) 1.7549]\n",
      "Whither, thou?\n",
      "Your firke onic hattar's have, lorst site;\n",
      "I strangess the meay, home fell this we sere \n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"hidden_size\": 50,\n",
    "    \"n_layers\": 2,\n",
    "    \"lr\": 0.005,\n",
    "    \"n_epochs\": 2000,\n",
    "    \"print_every\": 100,\n",
    "    \"plot_every\": 10,\n",
    "    \"hidden_size\": 100,\n",
    "    \"chunk_len\": 200,\n",
    "}\n",
    "\n",
    "n_epochs = args[\"n_epochs\"]\n",
    "chunk_len = args[\"chunk_len\"]\n",
    "\n",
    "decoder = RNN(n_characters, args[\"hidden_size\"], n_characters, args[\"n_layers\"])\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=args[\"lr\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set(chunk_len))\n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % args[\"print_every\"] == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (elapsed(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "    if epoch % args[\"plot_every\"] == 0:\n",
    "        all_losses.append(loss_avg / args[\"plot_every\"])\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
