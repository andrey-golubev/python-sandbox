{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ансамбли решающих функций (классификаторов)\n",
    "\n",
    "На сегодняшний день существуют два подхода:\n",
    "    \n",
    "* Баггинг (построение независимых классификаторов):\n",
    "        - Bagging\n",
    "        - Random Forests\n",
    "        - ...\n",
    "* Бустинг (новые классификаторы знают об ошибках других и учатся на их ошибках):\n",
    "        - AdaBoost\n",
    "        - Gradient Boosting\n",
    "        - ...\n",
    "\n",
    "\n",
    "Dataset: [armenian pubs](https://www.kaggle.com/erikhambardzumyan/pubs)\n",
    "\n",
    "Подробнее о данных можно узнать в [первой работе](https://github.com/andrgolubev/python-sandbox/blob/master/data_analysis_1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Плюсы и минусы ансамблей деревьев решений\n",
    "\n",
    "** Плюсы **:\n",
    "\n",
    "* Все достоинства деревьев решений сохраняются (кроме возможности интерпретации решающего правила - но остается возможность определения значимости признаков)\n",
    "* Хорошее (часто рекордное) качество предсказания\n",
    "* Random Forest (почти) не переобучается (GBT, AdaBoost - чуть похуже)\n",
    "\n",
    "** Минусы **:\n",
    "\n",
    "* GBT, AdaBoost - много параметров, которые надо подбирать\n",
    "* Медленно обучаются (особенно GBT, который требует обычно много деревьев)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 53\n"
     ]
    }
   ],
   "source": [
    "import data_retrieve as dr\n",
    "X, data = dr.parse_and_prepare(\"armenian_pubs.csv\", sep=\",\", engine=\"python\")\n",
    "Y = None\n",
    "Y_numeric = data['wts']\n",
    "Y_mean = np.mean(Y_numeric)\n",
    "Y = pd.Series(['C1' if val <= Y_mean else 'C2' for val in Y_numeric])\n",
    "intY = pd.Series([0 if val is 'C1' else 1 for val in Y])\n",
    "feature_names = X.columns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# constant random_state to have repeatability\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.7, random_state=7)\n",
    "N_train, _ = X_train.shape\n",
    "N_test, _ = X_test.shape\n",
    "intY = pd.Series([0 if val is 'C1' else 1 for val in Y_train])\n",
    "intY_test = pd.Series([0 if val is 'C1' else 1 for val in Y_test])\n",
    "print(N_train, N_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Ансамбль параллельно обучаемых «независимых» деревьев решений.\n",
    "\n",
    "Независимое построение определенного количества $M = {}$`n_estimators` (по умолчанию, 10) деревьев:\n",
    "\n",
    "Генерация случайной bootstrap-подвыборки (по умолчанию, ее мощность равна $\\sqrt{N}$) из обучающей выборки и построение дерева решений по данной подвыборке (в каждом\n",
    "новом узле дерева переменная для разбиения выбирается не из всех признаков, а из\n",
    "случайно выбранного их подмножества небольшой мощности)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=700, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=700)\n",
    "rf_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.528301886792\n"
     ]
    }
   ],
   "source": [
    "err_train = np.mean(Y_train != rf_model.predict(X_train))\n",
    "err_test = np.mean(Y_test != rf_model.predict(X_test))\n",
    "print(err_train, err_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "Будем использовать веса $w_1,w_2, \\dots ,w_N$.\n",
    "\n",
    "На первой итерации $w_i = 1/N$ ($i = 1, 2, \\dots ,N$) и алгоритм построения $f_1$ работает в\n",
    "обычном режиме.\n",
    "На $m$-й итерации увеличиваются веса тех прецедентов, на которых на $(m − 1)$-й\n",
    "итерации была допущена ошибка, и уменьшаются веса тех прецедентов, которые на\n",
    "предыдущей итерации были классифицированы правильно.\n",
    "\n",
    "На $m$-й итерации ищем классификатор $f_m$, минимизирующий ошибку\n",
    "$$\n",
    "{\\rm err}_m = \\sum_{f_m(x^{(i)}) \\ne y^{(i)}} w_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=700, random_state=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adab_model = AdaBoostClassifier(n_estimators=700)\n",
    "adab_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.139344262295 0.509433962264\n"
     ]
    }
   ],
   "source": [
    "err_train = np.mean(Y_train != adab_model.predict(X_train))\n",
    "err_test = np.mean(Y_test != adab_model.predict(X_test))\n",
    "print(err_train, err_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "На каждой итерации строится новый классификатор, аппроксимирующий значение градиента функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=700, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gdb_model = GradientBoostingClassifier(n_estimators=700)\n",
    "gdb_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.547169811321\n"
     ]
    }
   ],
   "source": [
    "err_train = np.mean(Y_train != gdb_model.predict(X_train))\n",
    "err_test = np.mean(Y_test != gdb_model.predict(X_test))\n",
    "print(err_train, err_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1538cee0791436aab79e2dca66a3124"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSWqMxS1JjbG4JakxFrckNcbilqTG/D+Ck9r2PhEEGwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9790e1ab00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "defines = {\n",
    "    'random_forest': {\n",
    "        'ctor': RandomForestClassifier\n",
    "    },\n",
    "    'ada_boost': {\n",
    "        'ctor': AdaBoostClassifier\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'ctor': GradientBoostingClassifier\n",
    "    }\n",
    "}\n",
    "\n",
    "names = X.columns\n",
    "_, n_features = X.shape\n",
    "def print_svc_data(model='random_forest', graph='hist', n_estimators=700):\n",
    "    model_defines = defines[model]\n",
    "    tree_model = model_defines['ctor'](n_estimators=n_estimators)\n",
    "    tree_model.fit(X_train, Y_train)\n",
    "    Y_train_predict = tree_model.predict(X_train)\n",
    "    Y_test_predict = tree_model.predict(X_test)\n",
    "    err_train = np.mean(Y_train != Y_train_predict)\n",
    "    err_test = np.mean(Y_test != Y_test_predict)\n",
    "    print(f\"Errors: train - {err_train} | test - {err_test}\")\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_train = confusion_matrix(Y_train_predict, Y_train)\n",
    "    print(\"Train data Confusion Matrix:\\n\", cm_train)\n",
    "    cm_test = confusion_matrix(Y_test_predict, Y_test)\n",
    "    print(\"Test data Confusion Matrix:\\n\", cm_test)\n",
    "    \n",
    "    tnr_tr = 1 - float(cm_train[1, 0])/(cm_train[0, 0] + cm_train[1, 0])\n",
    "    tpr_tr = 1 - float(cm_train[0, 1])/(cm_train[0, 1] + cm_train[1, 1])\n",
    "    print(f\"Train data: Specificity={tnr_tr} | Sensitivity={tpr_tr}\")\n",
    "    tnr_tst = 1 - float(cm_test[1, 0])/(cm_test[0, 0] + cm_test[1, 0])\n",
    "    tpr_tst = 1 - float(cm_test[0, 1])/(cm_test[0, 1] + cm_test[1, 1])\n",
    "    print(f\"Test data: Specificity={tnr_tst} | Sensitivity={tpr_tst}\")\n",
    "    probability = tree_model.predict_proba(X_test)[:, 1]\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, thresholds = roc_curve(intY_test, probability)\n",
    "    \n",
    "    importances = tree_model.feature_importances_\n",
    "    indices = np.argsort(importances)[:-11:-1]\n",
    "    print(\"Feature ranking:\")\n",
    "    for f in range(10):\n",
    "        print(\"%2d. feature '%5s' (%f)\" % (f + 1, names[indices[f]], importances[indices[f]]))\n",
    "    \n",
    "    if graph is 'hist':\n",
    "        plt.hist(probability[Y_test == 'C1'], color = 'b', alpha = 0.4, normed = False)\n",
    "        plt.hist(probability[Y_test == 'C2'], color = 'r', alpha = 0.4, normed = False)\n",
    "        pass\n",
    "    else:\n",
    "        from sklearn.metrics import roc_curve\n",
    "        fpr, tpr, thresholds = roc_curve(intY_test, probability)\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.plot(np.concatenate(([0], fpr)), np.concatenate(([0], tpr)))\n",
    "        #plt.plot(fpr, tpr)\n",
    "        plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "        plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.axis([-0.05, 1.05, -0.05, 1.05])\n",
    "        from sklearn.metrics import auc\n",
    "        print(\"Area Under Curve: \", auc(fpr, tpr))\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "interact(print_svc_data, model=defines.keys(), graph=['hist', 'roc'], n_estimators=(1, 2500, 1))\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
